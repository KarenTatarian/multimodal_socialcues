# multimodal_socialcues
Author: Karen Tatarian

Multi-modal Social Cues for Socially Intelligent Human-Robot Interaction 

This project is part of the following publication:  "How does modality matter? Investigating the synthesis and effects of multi-modal robot behavior on social intelligence". 
This work presents a multi-modal interaction focusing on the following modalities: proxemics for social navigation, gaze mechanisms (for turn-taking floor-holding, turn-yielding and joint attention), kinesics (for symbolic, deictic, and beat gestures), and social verbal content.

This application was implemented on Pepper robot (by SoftBank Robotics) running on Android. 

This version is compatible with Pepper running Naoqi OS 2.9.5.172 and more.

In this application, Pepper acts a travel agent planning a holiday with the user. 

# Getting Started

## Prerequisites
A robotified project for Pepper with QiSDK. Read the [documentation](https://developer.softbankrobotics.com/pepper-qisdk) if needed. 

## Running the Application 
The project comes complete with a sample project. You can clone the repository, open it in Android Studio, and run this directly onto a robot.

## Project Content 

